{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain pinecone-client tiktoken langchain-community openai spacy sentence-transformers ace_tools pandas langchain-openai langchain-pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created: 3\n",
      "Total token count: 926\n"
     ]
    }
   ],
   "source": [
    "# Block 1: Chunking Your Data\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "import json\n",
    "import tiktoken\n",
    "import requests\n",
    "\n",
    "max_tokens = 1000  # Maximum tokens per chunk\n",
    "overlap_tokens = 100  # Tokens to overlap between chunks\n",
    "\n",
    "url = \"https://chatwithaudio.blob.core.windows.net/uploads/kp_07d5b59a68fe4b2285ed52c6ad1e32eb/transcription_5fd87ab0-4c85-49d3-9bd1-65103f1234f3.json?sp=r&st=2024-10-14T19:47:42Z&se=2024-10-15T03:47:42Z&spr=https&sv=2022-11-02&sr=b&sig=HZrdE5Umj6Cx5Ruhiqyu2ZRRamvtYrrd2Jnc9xpn08o%3D\"\n",
    "response = requests.get(url)\n",
    "requestJson = response.json()\n",
    "\n",
    "documents = []\n",
    "for paragraph in requestJson.get('paragraphs', []):\n",
    "    for sentence in paragraph.get('sentences', []):\n",
    "        doc = Document(\n",
    "            page_content=sentence['text'],\n",
    "            metadata={\n",
    "                \"start\": sentence['start'],\n",
    "                \"end\": sentence['end'],\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "\n",
    "def count_tokens(text, model_name='text-embedding-ada-002'):\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "sentences = []\n",
    "for doc in documents:\n",
    "    text = doc.page_content\n",
    "    tokens = count_tokens(text)\n",
    "    sentences.append({'text': text, 'tokens': tokens, 'metadata': doc.metadata})\n",
    "\n",
    "\n",
    "chunks = []\n",
    "current_chunk = []\n",
    "current_token_count = 0\n",
    "\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    sentence_tokens = sentence['tokens']\n",
    "    if current_token_count + sentence_tokens > max_tokens:\n",
    "        chunk_text = ' '.join([s['text'] for s in current_chunk])\n",
    "        chunk_metadata = {\n",
    "            'start': current_chunk[0]['metadata']['start'],\n",
    "            'end': current_chunk[-1]['metadata']['end'],\n",
    "        }\n",
    "        chunk_doc = Document(page_content=chunk_text, metadata=chunk_metadata)\n",
    "        chunks.append(chunk_doc)\n",
    "\n",
    "        # Start new chunk with overlap\n",
    "        overlap = []\n",
    "        overlap_token_count = 0\n",
    "        i = len(current_chunk) - 1\n",
    "        while i >= 0 and overlap_token_count < overlap_tokens:\n",
    "            overlap.insert(0, current_chunk[i])\n",
    "            overlap_token_count += current_chunk[i]['tokens']\n",
    "            i -= 1\n",
    "        current_chunk = overlap.copy()\n",
    "        current_token_count = overlap_token_count\n",
    "    current_chunk.append(sentence)\n",
    "    current_token_count += sentence_tokens\n",
    "\n",
    "if current_chunk:\n",
    "    chunk_text = ' '.join([s['text'] for s in current_chunk])\n",
    "    chunk_metadata = {\n",
    "        'start': current_chunk[0]['metadata']['start'],\n",
    "        'end': current_chunk[-1]['metadata']['end'],\n",
    "    }\n",
    "    chunk_doc = Document(page_content=chunk_text, metadata=chunk_metadata)\n",
    "    chunks.append(chunk_doc)\n",
    "\n",
    "print(f\"Number of chunks created: {len(chunks)}\")\n",
    "print(f\"Total token count: {current_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pinecone client setup complete\n"
     ]
    }
   ],
   "source": [
    "# Block 2.1: Setting Up Pinecone with the simplified setup\n",
    "\n",
    "from pinecone import Pinecone\n",
    "import os\n",
    "\n",
    "# NOTE: there was an update and now the Pinecone client requires the following simplified setup\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "index_name = \"chatbot\"\n",
    "pinecone_index = pc.Index(index_name)\n",
    "\n",
    "print(\"pinecone client setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully inserted into Pinecone for project your-project-1234.\n"
     ]
    }
   ],
   "source": [
    "# Block 2.2: Embed and upsert data into Pinecone\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "project_id = \"your-project-1234\"\n",
    "\n",
    "# create batch embeddings\n",
    "batch_size = 100 \n",
    "all_embeddings = []\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "texts = [chunk.page_content for chunk in chunks]\n",
    "metadatas = [chunk.metadata for chunk in chunks]\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch_texts = texts[i:i+batch_size]\n",
    "    batch_embeddings = embeddings.embed_documents(batch_texts)\n",
    "    all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "# Upsert vectors into Pinecone in batches, specifying the namespace\n",
    "vectors = []\n",
    "for i, (embedding, metadata) in enumerate(zip(all_embeddings, metadatas)):\n",
    "    vector_id = f\"vec_{i}\"\n",
    "    metadata['text'] = texts[i]\n",
    "    vectors.append({'id': vector_id, 'values': embedding, 'metadata': metadata})\n",
    "\n",
    "for i in range(0, len(vectors), batch_size):\n",
    "    batch = vectors[i:i+batch_size]\n",
    "    # Upsert the batch into the specified namespace (project_id)\n",
    "    pinecone_index.upsert(vectors=batch, namespace=project_id)\n",
    "\n",
    "print(f\"Data successfully inserted into Pinecone for project {project_id}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Me\\Desktop\\chat-with-audio\\venv\\lib\\site-packages\\langchain_community\\vectorstores\\pinecone.py:68: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Block 3: Implementing Retrieval Augmented Generation (RAG)\n",
    "\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "vectorstore = Pinecone(pinecone_index, embeddings.embed_query, \"text\", namespace=project_id)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "The answer should be short, concise and directly related to the question and not contain filler words. \n",
    "Given the following information, answer the question. \n",
    "Use the information from the documents to support your answer. \n",
    "Do not use any external information or make up any information. \n",
    "If you don't know the answer, write \"I don't know\".\n",
    "\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # You can also try \"map_reduce\" or \"refine\" if needed\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Me\\AppData\\Local\\Temp\\ipykernel_28372\\3473050761.py:5: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Identify your target audience, create a compelling value proposition, leverage social media and online communities, offer free trials or demos, and network within relevant industry events.\n",
      "\n",
      "\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "How can i get my first saas customer?\n",
    "\"\"\"\n",
    "\n",
    "result = qa_chain(query)\n",
    "print(\"Answer:\", result['result'])\n",
    "\n",
    "print(\"\\n\\n---------------------------------------------------------\")\n",
    "for doc in result['source_documents']:\n",
    "    print(f\"Text: {doc.page_content[:200]}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    # print(f\"Metadata: {doc}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.6792088747024536, 'start': 260.55, 'end': 272.83502}, {'score': 0.6435611248016357, 'start': 503.95, 'end': 509.23}, {'score': 0.6272023916244507, 'start': 383.275, 'end': 401.5}, {'score': 0.6216945648193359, 'start': 487.55002, 'end': 495.835}, {'score': 0.6165946125984192, 'start': 45.145, 'end': 59.13}, {'score': 0.5801970958709717, 'start': 479.71002, 'end': 484.99002}, {'score': 0.5770946145057678, 'start': 140, 'end': 161.015}, {'score': 0.5500840544700623, 'start': 525.14496, 'end': 534.1}, {'score': 0.5472249984741211, 'start': 22.015, 'end': 25.695}, {'score': 0.5417923927307129, 'start': 513.15, 'end': 521.165}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# Load the model for sentence embeddings\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Fetch the JSON data from a remote URL\n",
    "url = \"https://chatwithaudio.blob.core.windows.net/uploads/kp_07d5b59a68fe4b2285ed52c6ad1e32eb/transcription_5fd87ab0-4c85-49d3-9bd1-65103f1234f3.json?sp=r&st=2024-10-14T19:47:42Z&se=2024-10-15T03:47:42Z&spr=https&sv=2022-11-02&sr=b&sig=HZrdE5Umj6Cx5Ruhiqyu2ZRRamvtYrrd2Jnc9xpn08o%3D\"\n",
    "response = requests.get(url)\n",
    "requestJson = response.json()\n",
    "\n",
    "sentences = []\n",
    "timestamps = []\n",
    "\n",
    "for paragraph in requestJson.get('paragraphs', []):\n",
    "    for sentence in paragraph.get('sentences', []):\n",
    "        sentences.append(sentence[\"text\"])\n",
    "        timestamps.append((sentence[\"start\"], sentence[\"end\"]))\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    return re.split(r'(?<=[.!?])\\s+', text)\n",
    "\n",
    "answer = result['result']\n",
    "answer_sentences = split_into_sentences(answer)\n",
    "\n",
    "text_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "all_matches = []\n",
    "\n",
    "for ans_sentence in answer_sentences:\n",
    "    ans_embedding = model.encode(ans_sentence, convert_to_tensor=True)\n",
    "    cosine_scores = util.cos_sim(ans_embedding, text_embeddings)[0]\n",
    "    top_results = torch.topk(cosine_scores, k=10)\n",
    "    \n",
    "    for idx, score in zip(top_results.indices, top_results.values):\n",
    "        all_matches.append({\n",
    "            'score': score.item(),\n",
    "            'start': timestamps[idx][0],\n",
    "            'end': timestamps[idx][1]\n",
    "        })\n",
    "\n",
    "all_matches = sorted(all_matches, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "top_matches = all_matches[:10]\n",
    "print(top_matches)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
