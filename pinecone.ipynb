{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Q langchain_community langchain_pinecone langchain_openai unstructured langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "import json\n",
    "import tiktoken\n",
    "import os\n",
    "\n",
    "data_folder = \"./data\"\n",
    "json_file = \"paragraphs.json\"\n",
    "file_path = os.path.join(data_folder, json_file)\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "documents = []\n",
    "for paragraph in data:\n",
    "    for sentence in paragraph['sentences']:\n",
    "        doc = Document(\n",
    "            page_content=sentence['text'],\n",
    "            metadata={\n",
    "                \"start\": sentence['start'],\n",
    "                \"end\": sentence['end'],\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "\n",
    "\n",
    "\n",
    "def count_tokens(text, model_name='text-embedding-ada-002'):\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "sentences = []\n",
    "for doc in documents:\n",
    "    text = doc.page_content\n",
    "    tokens = count_tokens(text)\n",
    "    sentences.append({'text': text, 'tokens': tokens, 'metadata': doc.metadata})\n",
    "\n",
    "max_tokens = 1024  # Maximum tokens per chunk\n",
    "overlap_tokens = 100  # Tokens to overlap between chunks\n",
    "\n",
    "chunks = []\n",
    "current_chunk = []\n",
    "current_token_count = 0\n",
    "\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    sentence_tokens = sentence['tokens']\n",
    "    if current_token_count + sentence_tokens > max_tokens:\n",
    "        # Create chunk\n",
    "        chunk_text = ' '.join([s['text'] for s in current_chunk])\n",
    "        chunk_metadata = {\n",
    "            'start': current_chunk[0]['metadata']['start'],\n",
    "            'end': current_chunk[-1]['metadata']['end'],\n",
    "        }\n",
    "        chunk_doc = Document(page_content=chunk_text, metadata=chunk_metadata)\n",
    "        chunks.append(chunk_doc)\n",
    "\n",
    "        # Start new chunk with overlap\n",
    "        overlap = []\n",
    "        overlap_token_count = 0\n",
    "        i = len(current_chunk) - 1\n",
    "        while i >= 0 and overlap_token_count < overlap_tokens:\n",
    "            overlap.insert(0, current_chunk[i])\n",
    "            overlap_token_count += current_chunk[i]['tokens']\n",
    "            i -= 1\n",
    "        current_chunk = overlap.copy()\n",
    "        current_token_count = overlap_token_count\n",
    "\n",
    "    # Add current sentence to current_chunk\n",
    "    current_chunk.append(sentence)\n",
    "    current_token_count += sentence_tokens\n",
    "\n",
    "# Add any remaining sentences as a chunk\n",
    "if current_chunk:\n",
    "    chunk_text = ' '.join([s['text'] for s in current_chunk])\n",
    "    chunk_metadata = {\n",
    "        'start': current_chunk[0]['metadata']['start'],\n",
    "        'end': current_chunk[-1]['metadata']['end'],\n",
    "    }\n",
    "    chunk_doc = Document(page_content=chunk_text, metadata=chunk_metadata)\n",
    "    chunks.append(chunk_doc)\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup pinecone\n",
    "from pinecone import Pinecone\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "dimension = 1536\n",
    "index_name = \"chatbot\"\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All vectors in the index 'chatbot' have been deleted.\n"
     ]
    }
   ],
   "source": [
    "# helper method to wipe out entire pinecone index\n",
    "index.delete(delete_all=True)\n",
    "print(f\"All vectors in the index '{index_name}' have been deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')  # Adjust model as needed\n",
    "\n",
    "# Extract texts and metadata\n",
    "texts = [chunk.page_content for chunk in chunks]\n",
    "metadatas = [chunk.metadata for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100  # Adjust based on your preference and API rate limits\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch_texts = texts[i:i+batch_size]\n",
    "    batch_metadatas = metadatas[i:i+batch_size]\n",
    "    batch_embeddings = embeddings.embed_documents(batch_texts)\n",
    "    \n",
    "    # Prepare vectors for upsert\n",
    "    vectors = []\n",
    "    for idx, embedding in enumerate(batch_embeddings):\n",
    "        vector_id = f'vec_{i + idx}'\n",
    "        metadata = batch_metadatas[idx]\n",
    "        metadata['text'] = batch_texts[idx]  # Include text in metadata if needed\n",
    "        vectors.append((vector_id, embedding, metadata))\n",
    "    \n",
    "    # Upsert vectors into Pinecone\n",
    "    index.upsert(vectors=vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query, top_k=5):\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    \n",
    "    # Query Pinecone\n",
    "    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "    \n",
    "    # Extract texts and metadata\n",
    "    retrieved_chunks = []\n",
    "    for match in results['matches']:\n",
    "        metadata = match['metadata']\n",
    "        text = metadata.get('text', '')\n",
    "        start = metadata.get('start', '')\n",
    "        end = metadata.get('end', '')\n",
    "        score = match['score']\n",
    "        retrieved_chunks.append({\n",
    "            'text': text,\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'score': score\n",
    "        })\n",
    "    return retrieved_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vec_86\n",
      "end: 486.77002\n",
      "start: 484.99002\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# helper method to view the contents of the index\n",
    "limit = 1\n",
    "fetch_response = index.query(\n",
    "    vector=[0] * dimension,\n",
    "    top_k=limit,\n",
    "    include_values=True,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "for match in fetch_response['matches']:\n",
    "    print(match[\"id\"])\n",
    "    for key, value in match['metadata'].items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "template = \"\"\"\n",
    "Given the following information, answer the question. Use the information from the documents to support your answer. Do not use any external information or make up any information. If you don't know the answer, write \"I don't know\".\n",
    "\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer: \n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the query\n",
    "question = \"How to get my first saas customers?\"\n",
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: To get your first SaaS customers, follow these steps:\n",
      "\n",
      "1. **Start Before Building the Product**: Begin by identifying any advantages you have before writing a single line of code. This includes considering if you have an existing audience or a network in a particular space that you can leverage.\n",
      "\n",
      "2. **Leverage Your Network**: Utilize your network to help promote your product. This can include calling on people you know to do webinars, co-promote, or otherwise help sell your product.\n",
      "\n",
      "3. **Choose a Marketing Approach**: Select one marketing approach that you believe is most likely to work for your product. This could be SEO, content marketing, cold outreach, partnerships, integrations, or pay-per-click ads.\n",
      "\n",
      "4. **Experiment and Commit**: Dive into your chosen marketing approach and commit to it for a few months. Experiment and give it your all to see if it can help you reach the 100 customer mark.\n",
      "\n",
      "By following these steps, you can strategically work towards acquiring your first 100 customers for your SaaS product.\n",
      "\n",
      "Source Documents:\n",
      "{'end': 22.015, 'num_words': 80.0, 'start': 0.08}\n",
      "---\n",
      "{'end': 59.13, 'num_words': 141.0, 'start': 22.015}\n",
      "---\n",
      "{'end': 509.23, 'num_words': 105.0, 'start': 479.71002}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# view the results\n",
    "print(\"Answer:\", result[\"result\"])\n",
    "print(\"\\nSource Documents:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(doc.metadata)\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
